# OpenAQ Air Quality Data Ingestion for Nepal, India & China

A comprehensive Python system for collecting air quality data from the OpenAQ API, specifically designed to gather data from Nepal and surrounding countries (India and China) for air quality prediction modeling.

## ğŸ¯ Project Overview

This system provides:
- **Full-scale data collection** from OpenAQ API v3
- **Multi-country coverage**: Nepal, India, and China
- **Production-ready pipeline** with error handling and logging
- **Efficient data storage** in compressed Parquet format
- **Comprehensive analysis tools** for data quality assessment
- **Prediction-ready datasets** for machine learning models

## ğŸ“Š Expected Data Collection

- **Countries**: 3 (Nepal, India, China)
- **Locations**: ~3,000+ monitoring stations
- **Sensors**: ~15,000+ air quality sensors
- **Measurements**: 10-50 million historical records
- **Parameters**: PM2.5, PM10, O3, NO2, SO2, CO, temperature, humidity
- **Time Range**: 2015-present (10+ years of data)
- **Storage**: 5-15 GB compressed data

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or: venv\Scripts\activate  # Windows

# Install dependencies
pip install pandas requests pyarrow matplotlib seaborn
```

### 2. Run Data Collection

```bash
# Quick test run (recommended first)
python launch_ingestion.py --test

# Full-scale collection (4-8 hours)
python launch_ingestion.py

# Analyze collected data
python launch_ingestion.py --analyze
```

## ğŸ“ Project Structure

```
openaq_ingestion/
â”œâ”€â”€ launch_ingestion.py          # ğŸš€ Main launcher script
â”œâ”€â”€ run_full_ingestion.py        # ğŸ­ Full-scale ingestion engine
â”œâ”€â”€ test_ingestion.py            # ğŸ§ª Test/validation script
â”œâ”€â”€ analyze_results.py           # ğŸ“Š Data analysis tools
â”œâ”€â”€ config.py                    # âš™ï¸ Configuration settings
â”œâ”€â”€ ingest_openaq_data.py        # ğŸ“ Original development script
â”œâ”€â”€ README.md                    # ğŸ“– This documentation
â”œâ”€â”€ venv/                        # ğŸ Python virtual environment
â”œâ”€â”€ test_data/                   # ğŸ§ª Test ingestion outputs
â”‚   â”œâ”€â”€ test_locations_*.parquet
â”‚   â”œâ”€â”€ test_sensors_*.parquet
â”‚   â””â”€â”€ test_measurements_*.parquet
â””â”€â”€ full_data/                   # ğŸ­ Full ingestion outputs
    â”œâ”€â”€ all_locations_*.parquet
    â”œâ”€â”€ all_sensors_*.parquet
    â”œâ”€â”€ measurements_batch_*.parquet
    â”œâ”€â”€ ingestion_summary_*.json
    â””â”€â”€ data_analysis_report_*.txt
```

## ğŸ› ï¸ Script Descriptions

### Core Scripts

1. **`launch_ingestion.py`** - Smart launcher with environment checks
   - Validates dependencies and setup
   - Provides resource estimates
   - Offers test, full, and analysis modes
   - User-friendly interface with confirmations

2. **`run_full_ingestion.py`** - Production ingestion engine
   - Multi-threaded data collection
   - Comprehensive error handling and retries
   - Progress tracking and logging
   - Memory-efficient batch processing
   - Automatic data validation

3. **`test_ingestion.py`** - Quick validation script
   - Limited data collection for testing
   - Validates API connectivity
   - Tests data pipeline
   - ~30 minutes runtime

4. **`analyze_results.py`** - Data analysis and quality assessment
   - Comprehensive data statistics
   - Parameter availability analysis
   - Prediction readiness assessment
   - Geographic and temporal coverage
   - Data quality reporting

### Configuration

5. **`config.py`** - Centralized configuration
   - API settings and rate limiting
   - Country selection
   - Performance tuning
   - Output formatting options

## ğŸ“Š Data Schema

### Locations Data
```
all_locations_YYYYMMDD_HHMMSS.parquet
â”œâ”€â”€ id                   # Location ID
â”œâ”€â”€ name                 # Location name
â”œâ”€â”€ locality             # City/region
â”œâ”€â”€ country              # Country details (name, code, ID)
â”œâ”€â”€ coordinates          # Latitude, longitude
â”œâ”€â”€ timezone             # Local timezone
â”œâ”€â”€ sensors              # Associated sensors list
â”œâ”€â”€ owner                # Data owner info
â”œâ”€â”€ provider             # Data provider info
â”œâ”€â”€ datetimeFirst        # First measurement date
â”œâ”€â”€ datetimeLast         # Last measurement date
â”œâ”€â”€ isMobile             # Mobile vs fixed station
â””â”€â”€ isMonitor            # Monitoring station flag
```

### Sensors Data
```
all_sensors_YYYYMMDD_HHMMSS.parquet
â”œâ”€â”€ sensor_id            # Unique sensor ID
â”œâ”€â”€ sensor_name          # Sensor name
â”œâ”€â”€ parameter            # Parameter details (name, units, display)
â”œâ”€â”€ location_id          # Parent location ID
â”œâ”€â”€ location_name        # Location name
â”œâ”€â”€ country              # Country details
â”œâ”€â”€ coordinates          # Geographic coordinates
â”œâ”€â”€ timezone             # Local timezone
â”œâ”€â”€ owner                # Data owner
â”œâ”€â”€ provider             # Data provider
â””â”€â”€ datetime_first/last  # Temporal coverage
```

### Measurements Data
```
measurements_batch_XXXX_YYYYMMDD_HHMMSS.parquet
â”œâ”€â”€ date                 # Measurement timestamp (ISO format)
â”œâ”€â”€ value                # Measured value
â”œâ”€â”€ parameter            # Parameter type (pm25, o3, etc.)
â”œâ”€â”€ units                # Measurement units
â”œâ”€â”€ period               # Averaging period
â”œâ”€â”€ sensor_id            # Source sensor ID
â”œâ”€â”€ location_id          # Source location ID
â”œâ”€â”€ location_name        # Location name
â”œâ”€â”€ country_name         # Country name
â”œâ”€â”€ country_code         # Country code
â”œâ”€â”€ coordinates_lat      # Latitude
â”œâ”€â”€ coordinates_lon      # Longitude
â”œâ”€â”€ timezone             # Local timezone
â”œâ”€â”€ provider_name        # Data provider
â””â”€â”€ ingestion_timestamp  # When data was collected
```

## ğŸ”§ Configuration Options

Edit `config.py` to customize:

```python
# Performance settings
REQUEST_DELAY = 0.5          # Seconds between API requests
BATCH_SIZE = 50000           # Measurements per file
MAX_WORKERS = 3              # Concurrent requests

# Date range
DATE_FROM = "2015-01-01T00:00:00Z"
DATE_TO = None               # None = current time

# Countries (add/remove as needed)
TARGET_COUNTRIES = {
    "Nepal": 145,
    "India": 9,
    "China": 10
}
```

## ğŸ“ˆ Usage Examples

### Basic Usage

```bash
# Environment check and test run
python launch_ingestion.py --test

# Full data collection
python launch_ingestion.py

# Analyze existing data
python launch_ingestion.py --analyze
```

### Advanced Usage

```bash
# Run with custom config
python run_full_ingestion.py

# Direct analysis of specific data
python analyze_results.py full_data

# Environment check only
python launch_ingestion.py --skip-check
```

### Programmatic Usage

```python
# Using the ingestion classes directly
from run_full_ingestion import FullScaleOpenAQIngester

api_key = "your_api_key_here"
ingester = FullScaleOpenAQIngester(api_key, "my_output_dir")
ingester.run_full_ingestion()

# Analysis
from analyze_results import OpenAQDataAnalyzer

analyzer = OpenAQDataAnalyzer("my_output_dir")
analyzer.run_complete_analysis()
```

## ğŸš¦ Performance and Monitoring

### Monitoring Progress

The ingestion provides real-time progress updates:
- Countries processed
- Locations and sensors collected
- Measurements ingested
- Estimated completion time
- Current processing status

### Log Files

Comprehensive logging includes:
- API request/response details
- Error tracking and retries
- Performance metrics
- Progress checkpoints
- Final statistics

### Resource Usage

- **CPU**: Light (I/O bound operations)
- **Memory**: 2-4 GB recommended
- **Network**: ~10,000 API requests
- **Storage**: 5-15 GB for full dataset

## ğŸ” Data Quality Features

### Built-in Validation
- Parameter standardization
- Geographic coordinate validation
- Timestamp consistency checks
- Unit conversion verification
- Duplicate detection

### Analysis Capabilities
- Data completeness scoring
- Parameter availability matrix
- Temporal coverage analysis
- Geographic distribution mapping
- Prediction readiness assessment

## ğŸ¤– Machine Learning Ready

The collected data is optimized for:

### Time Series Forecasting
- Hourly measurement resolution
- Multi-year historical data
- Consistent temporal indexing
- Missing data flagging

### Spatial Modeling
- Dense sensor networks
- Geographic coordinates
- Administrative boundaries
- Cross-border coverage

### Multi-parameter Prediction
- Correlated pollutants (PM2.5, PM10, O3)
- Meteorological data (temperature, humidity)
- Source attribution parameters
- Air quality indices

## âš¡ API Rate Limiting

The system implements responsible API usage:
- 0.5-1 second delays between requests
- Automatic retry with exponential backoff
- Rate limit detection and handling
- Respectful concurrent request limits

## ğŸ›¡ï¸ Error Handling

Robust error management includes:
- Network timeout recovery
- HTTP error handling
- JSON parsing validation
- Partial data preservation
- Graceful degradation

## ğŸ“ Output Files

### Data Files
- **Parquet format**: Efficient, compressed, cross-platform
- **Batch organization**: Memory-efficient processing
- **Metadata preservation**: Full context retention
- **Schema consistency**: Standardized column structure

### Reports
- **Ingestion summary**: JSON metadata
- **Analysis report**: Comprehensive text report
- **Log files**: Detailed operation logs
- **Progress tracking**: Real-time status updates

## ğŸ”® Future Enhancements

Potential improvements:
- Real-time data streaming
- Automated data updates
- Advanced quality control
- Integration with prediction models
- Web-based monitoring dashboard
- Data visualization tools

## ğŸ› Troubleshooting

### Common Issues

1. **API Key Issues**
   ```bash
   # Verify API key in config.py
   # Check OpenAQ account status
   ```

2. **Network Problems**
   ```bash
   # Check internet connectivity
   # Verify firewall settings
   # Test with --test flag first
   ```

3. **Memory Issues**
   ```bash
   # Reduce BATCH_SIZE in config.py
   # Close other applications
   # Monitor with system tools
   ```

4. **Disk Space**
   ```bash
   # Check available space (5-15 GB needed)
   # Clean old data files
   # Use external storage if needed
   ```

### Debug Mode

Enable detailed logging:
```python
# In config.py
LOG_LEVEL = "DEBUG"
SAVE_RAW_RESPONSES = True
```

## ğŸ“ Support

For issues or questions:
1. Check log files for error details
2. Run test ingestion first
3. Verify environment setup
4. Review configuration settings
5. Check OpenAQ API status

## ğŸ“„ License

This project is designed for educational and research purposes. Please respect OpenAQ's terms of service and rate limits.

---

**Happy Air Quality Monitoring! ğŸŒğŸ’¨ğŸ“Š**  
âœ… **File Output**: Parquet files generated correctly  
âœ… **Error Handling**: Graceful handling of API timeouts and server errors  

## Usage Instructions

### 1. Setup (Already Done)
```bash
python3 -m venv venv
source venv/bin/activate
pip install requests pandas pyarrow
```

### 2. Run Full Ingestion (Will take several hours)
```bash
source venv/bin/activate
python ingest_openaq_data.py
```

### 3. Run Limited Test (Quick test with Nepal only)
```bash
source venv/bin/activate  
python test_ingestion.py
```

## Output Files

- `locations_TIMESTAMP.parquet`: All location data for Nepal, India, China
- `sensors_TIMESTAMP.parquet`: All sensor data with location context
- `measurements_*.parquet`: All measurement data (may be split into batches)
- `*.log`: Detailed logging of the ingestion process

## API Details

- **Base URL**: https://api.openaq.org/v3
- **Country IDs**: Nepal=145, India=9, China=10
- **Rate Limiting**: 1 second between requests
- **Pagination**: Handled automatically
- **Error Handling**: Robust with retry logic and logging

## Data Structure

1. **Locations**: Geographic points with sensors
2. **Sensors**: Individual measurement devices at locations  
3. **Measurements**: Time-series data from sensors

## Expected Data Volume

Based on test results:
- **Nepal alone**: 14 locations, 66 sensors
- **Test sample**: 31,000 measurements from just 2 sensors
- **Full dataset estimate**: Several million measurement records
- **File sizes**: Gigabytes when complete

## Performance Notes

- **Test ingestion**: ~4 minutes for 2 sensors (31,000 measurements)
- **Full ingestion estimate**: 4-8 hours depending on API performance
- **Memory management**: Batches data to avoid memory issues
- **Error resilience**: Continues processing even if some API calls fail

## Production Recommendations

1. **API Key Security**: Move API key to environment variable
2. **Incremental Updates**: Modify to only fetch new data since last run
3. **Monitoring**: Add alerting for failed ingestions
4. **Storage**: Consider database storage for large datasets
5. **Scheduling**: Set up cron job for regular data updates